{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/fc/n5827zt91vb65jnqcsgpv5cr0000gn/T/ipykernel_14419/737780644.py:256: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 21:38:19.428500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_path_v2/variables/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 21:38:22.695542: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "#@title Import libraries\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "from easydict import EasyDict\n",
    "import collections\n",
    "import json\n",
    "import yaml\n",
    "import os.path as osp\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "import tensorflow.compat.v1 as tf # version .1\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "\n",
    "\n",
    "#@title Define hyperparameters\n",
    "FLAGS = {\n",
    "    'prompt_engineering': True,\n",
    "    'this_is': True,\n",
    "    \n",
    "    'temperature': 100.0,\n",
    "    'use_softmax': False,\n",
    "}\n",
    "FLAGS = EasyDict(FLAGS)\n",
    "\n",
    "# Global matplotlib settings\n",
    "SMALL_SIZE = 16#10\n",
    "MEDIUM_SIZE = 18#12\n",
    "BIGGER_SIZE = 20#14\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# Parameters for drawing figure.\n",
    "display_input_size = (10, 10)\n",
    "overall_fig_size = (18, 24)\n",
    "line_thickness = 2\n",
    "fig_size_w = 35\n",
    "# fig_size_h = min(max(5, int(len(category_names) / 2.5) ), 10)\n",
    "mask_color =   'red'\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def article(name):  #% A/An\n",
    "  return 'an' if name[0] in 'aeiou' else 'a'\n",
    "\n",
    "def processed_name(name, rm_dot=False):  #% '_' for lvis, '/' for obj365\n",
    "  res = name.replace('_', ' ').replace('/', ' or ').lower()\n",
    "  if rm_dot:\n",
    "    res = res.rstrip('.')\n",
    "  return res\n",
    "\n",
    "\n",
    "# //////////////////////////////////////////////////////////////\n",
    "single_template = [  #% Only one\n",
    "    'a photo of {article} {}.'\n",
    "]\n",
    "# //////////////////////////////////////////////////////////////\n",
    "multiple_templates = [\n",
    "    'There is {article} {} in the scene.',\n",
    "    'There is the {} in the scene.',\n",
    "    'a photo of {article} {} in the scene.',\n",
    "    'a photo of the {} in the scene.',\n",
    "    'a photo of one {} in the scene.',\n",
    "\n",
    "    'itap of {article} {}.',\n",
    "    'itap of my {}.',  # itap: I took a picture of\n",
    "    'itap of the {}.',\n",
    "    \n",
    "    'a photo of {article} {}.',\n",
    "    'a photo of my {}.',\n",
    "    'a photo of the {}.',\n",
    "    'a photo of one {}.',\n",
    "    'a photo of many {}.',\n",
    "\n",
    "    'a good photo of {article} {}.',\n",
    "    'a good photo of the {}.',\n",
    "    'a bad photo of {article} {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a photo of a nice {}.',\n",
    "    'a photo of the nice {}.',\n",
    "    'a photo of a cool {}.',\n",
    "    'a photo of the cool {}.',\n",
    "    'a photo of a weird {}.',\n",
    "    'a photo of the weird {}.',\n",
    "\n",
    "    'a photo of a small {}.',\n",
    "    'a photo of the small {}.',\n",
    "    'a photo of a large {}.',\n",
    "    'a photo of the large {}.',\n",
    "\n",
    "    'a photo of a clean {}.',\n",
    "    'a photo of the clean {}.',\n",
    "    'a photo of a dirty {}.',\n",
    "    'a photo of the dirty {}.',\n",
    "\n",
    "    'a bright photo of {article} {}.',\n",
    "    'a bright photo of the {}.',\n",
    "    'a dark photo of {article} {}.',\n",
    "    'a dark photo of the {}.',\n",
    "\n",
    "    'a photo of a hard to see {}.',\n",
    "    'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of {article} {}.',\n",
    "    'a low resolution photo of the {}.',\n",
    "    'a cropped photo of {article} {}.',\n",
    "    'a cropped photo of the {}.',\n",
    "    'a close-up photo of {article} {}.',\n",
    "    'a close-up photo of the {}.',\n",
    "    'a jpeg corrupted photo of {article} {}.',\n",
    "    'a jpeg corrupted photo of the {}.',\n",
    "    'a blurry photo of {article} {}.',\n",
    "    'a blurry photo of the {}.',\n",
    "    'a pixelated photo of {article} {}.',\n",
    "    'a pixelated photo of the {}.',\n",
    "\n",
    "    'a black and white photo of the {}.',\n",
    "    'a black and white photo of {article} {}.',\n",
    "\n",
    "    'a plastic {}.',\n",
    "    'the plastic {}.',\n",
    "\n",
    "    'a toy {}.',\n",
    "    'the toy {}.',\n",
    "    'a plushie {}.',\n",
    "    'the plushie {}.',\n",
    "    'a cartoon {}.',\n",
    "    'the cartoon {}.',\n",
    "\n",
    "    'an embroidered {}.',\n",
    "    'the embroidered {}.',\n",
    "\n",
    "    'a painting of the {}.',\n",
    "    'a painting of a {}.',\n",
    "]\n",
    "# //////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clip.available_models()\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#///////////////////////////////////////////\n",
    "#     Build text embedding function:\n",
    "def build_text_embedding(categories):\n",
    "  if FLAGS.prompt_engineering:  ## prompt engineering => some templates\n",
    "    templates = multiple_templates\n",
    "  else:\n",
    "    templates = single_template\n",
    "\n",
    "  with torch.no_grad():\n",
    "    all_text_embeddings = []  ## All Text Embeddings\n",
    "    print('Building text embeddings...')\n",
    "    \n",
    "    for category in tqdm(categories):\n",
    "      texts = [\n",
    "        template.format(processed_name(category['name'], rm_dot=True),\n",
    "                        article=article(category['name']))\n",
    "        for template in templates\n",
    "      ]\n",
    "      if FLAGS.this_is:\n",
    "        texts = [\n",
    "                 'This is ' + text if text.startswith('a') or text.startswith('the') else text \n",
    "                 for text in texts\n",
    "                 ]\n",
    "      texts = clip.tokenize(texts) #@ Returns a LongTensor containing tokenized sequences of given text input(s).\n",
    "      #TODO print(\"texts[tokenized sequences of given text inputs]: \", texts) ## This can be used as the input to the model.\n",
    "      if torch.cuda.is_available():\n",
    "        texts = texts.cuda()\n",
    "      \n",
    "      #@ Given a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.  \n",
    "      text_embeddings = model.encode_text(texts) #embed with text encoder \n",
    "      text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "      text_embedding = text_embeddings.mean(dim=0)\n",
    "      text_embedding /= text_embedding.norm()\n",
    "      all_text_embeddings.append(text_embedding)\n",
    "    ## FOR ENDING  \n",
    "    all_text_embeddings = torch.stack(all_text_embeddings, dim=1)\n",
    "    if torch.cuda.is_available():\n",
    "      all_text_embeddings = all_text_embeddings.cuda()\n",
    "  ## WITH ENDING  \n",
    "  return all_text_embeddings.cpu().numpy().T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Helper Function ##\n",
    "numbered_categories = [{'name': str(idx), 'id': idx,} for idx in range(50)]\n",
    "# print(numbered_categories)  # [{'name': '0', 'id': 0}, {, ..., {'name': '49', 'id': 49}]\n",
    "numbered_category_indices = {cat['id']: cat for cat in numbered_categories}\n",
    "# print(numbered_category_indices)  # 0: {'name': '0', 'id': 0}, ..., 49: {'name': '49', 'id': 49}}\n",
    "\n",
    "#@ Non Maximum Suppression\n",
    "#@ dets: [N, 4] \n",
    "#@ scores: [N,]\n",
    "#@ thresh[Float]: iou threshold.\n",
    "#@ max_dets[Int]\n",
    "def nms(dets, scores, thresh, max_dets=1000):\n",
    "  y1 = dets[:, 0]\n",
    "  x1 = dets[:, 1]\n",
    "  y2 = dets[:, 2]\n",
    "  x2 = dets[:, 3]\n",
    "\n",
    "  areas = (x2 - x1) * (y2 - y1)\n",
    "  order = scores.argsort()[::-1]\n",
    "\n",
    "  keep = []\n",
    "  while order.size > 0 and len(keep) < max_dets:\n",
    "    i = order[0]\n",
    "    keep.append(i)\n",
    "\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "    w = np.maximum(0.0, xx2 - xx1)\n",
    "    h = np.maximum(0.0, yy2 - yy1)\n",
    "    intersection = w * h\n",
    "    overlap = intersection / (areas[i] + areas[order[1:]] - intersection + 1e-12)\n",
    "\n",
    "    inds = np.where(overlap <= thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "  ## WHILE ENDING \n",
    "  return keep\n",
    "\n",
    "\n",
    "## Graph and Session Init ##\n",
    "session = tf.Session(graph=tf.Graph())\n",
    "saved_model_dir = './image_path_v2' #@param {type:\"string\"}\n",
    "_ = tf.saved_model.loader.load(session, ['serve'], saved_model_dir) # Restoring parameters from ./image_path_v2/variables/variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def paste_instance_masks(masks, detected_boxes, image_height, image_width):\n",
    "  def expand_boxes(boxes, scale):\n",
    "    \"\"\"Expands an array of boxes by a given scale.\"\"\"\n",
    "    w_half = boxes[:, 2] * .5\n",
    "    h_half = boxes[:, 3] * .5\n",
    "    x_c = boxes[:, 0] + w_half\n",
    "    y_c = boxes[:, 1] + h_half\n",
    "\n",
    "    w_half *= scale\n",
    "    h_half *= scale\n",
    "\n",
    "    boxes_exp = np.zeros(boxes.shape)\n",
    "    boxes_exp[:, 0] = x_c - w_half\n",
    "    boxes_exp[:, 2] = x_c + w_half\n",
    "    boxes_exp[:, 1] = y_c - h_half\n",
    "    boxes_exp[:, 3] = y_c + h_half\n",
    "\n",
    "    return boxes_exp\n",
    "  \n",
    "  _, mask_height, mask_width = masks.shape\n",
    "  scale = max((mask_width + 2.0) / mask_width, (mask_height + 2.0) / mask_height)\n",
    "\n",
    "  ref_boxes = expand_boxes(detected_boxes, scale)  ## Expand the boxes\n",
    "  ref_boxes = ref_boxes.astype(np.int32)\n",
    "  padded_mask = np.zeros((mask_height + 2, mask_width + 2), dtype=np.float32)\n",
    "  segms = []\n",
    "  for mask_ind, mask in enumerate(masks):\n",
    "    im_mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "    padded_mask[1:-1, 1:-1] = mask[:, :]  # Process mask inside bounding boxes.\n",
    "\n",
    "    ref_box = ref_boxes[mask_ind, :]\n",
    "    w = ref_box[2] - ref_box[0] + 1\n",
    "    h = ref_box[3] - ref_box[1] + 1\n",
    "    w = np.maximum(w, 1)\n",
    "    h = np.maximum(h, 1)\n",
    "\n",
    "    mask = cv2.resize(padded_mask, (w, h))\n",
    "    mask = np.array(mask > 0.5, dtype=np.uint8)\n",
    "\n",
    "    x_0 = min(max(ref_box[0], 0), image_width)\n",
    "    x_1 = min(max(ref_box[2] + 1, 0), image_width)\n",
    "    y_0 = min(max(ref_box[1], 0), image_height)\n",
    "    y_1 = min(max(ref_box[3] + 1, 0), image_height)\n",
    "\n",
    "    im_mask[y_0:y_1, x_0:x_1] = mask[\n",
    "        (y_0 - ref_box[1]):(y_1 - ref_box[1]),\n",
    "        (x_0 - ref_box[0]):(x_1 - ref_box[0])\n",
    "    ]\n",
    "    segms.append(im_mask)\n",
    "\n",
    "  segms = np.array(segms)\n",
    "  assert masks.shape[0] == segms.shape[0]\n",
    "  \n",
    "  return segms   #@ Return numpy array [N, image_height, image_width] => the instance masks pasted on the image canvas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@title Visualization\n",
    "STANDARD_COLORS = ['AliceBlue', 'Chartreuse', 'Aqua', 'BlueViolet', 'DarkOrange', 'DeepPink', 'DeepSkyBlue', 'GhostWhite', 'Gold', 'LightBlue', 'LightCoral', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue', 'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'Olive', 'Orange', 'Purple', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown', 'SeaGreen', 'SeaShell', 'Sienna', 'Silver','Turquoise', 'Violet', 'Wheat', 'White', 'WhiteSmoke', 'Yellow', 'YellowGreen']\n",
    "  \n",
    "#* #########################################################\n",
    "#@ Draw_boxes function\n",
    "#@ image: a PIL.Image object.\n",
    "#@ display_str_list: list of strings to display in box (out of range => displaying below the box)\n",
    "#@ use_normalized_coordinates: True (default) => treat coordinates as relative to the image. Otherwise => absolute.\n",
    "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax, \n",
    "                               color='red', thickness=4,\n",
    "                               display_str_list=(),\n",
    "                               use_normalized_coordinates=True,user=0,):\n",
    "  if user==2 or user==3:\n",
    "    return\n",
    "  \n",
    "  draw = ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)\n",
    "  \n",
    "  try:\n",
    "    font = ImageFont.truetype('arial.ttf', 24)\n",
    "  except IOError:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "  #@ Displayed string position adjustment \n",
    "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)   # display_str has top and bottom margin_(0.05x).\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = bottom + total_display_str_height\n",
    "    \n",
    "  for display_str in display_str_list[::-1]:  \n",
    "    text_left = min(5, left)\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle([(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)], fill=color)\n",
    "    draw.rectangle([(left + text_width, text_bottom - text_height - 2 * margin), (right + margin, text_bottom)], fill='LightGreen')\n",
    "    \n",
    "    draw.text((left + margin, text_bottom - text_height - margin),display_str, fill='black', font=font)\n",
    "    #draw.text((left + margin + text_width*2, text_bottom - text_height - margin), display_str, fill='black', font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "\n",
    "#* #########################################################\n",
    "#@ Draw_box on image (numpy array form) {CALL draw_bounding_box_on_image()}\n",
    "#@ image: a numpy array with shape [height, width, 3].\n",
    "def draw_bounding_box_on_image_array(image, ymin, xmin, ymax, xmax, user=0,\n",
    "                                     color='red', thickness=4,\n",
    "                                     display_str_list=(),\n",
    "                                     use_normalized_coordinates=True):\n",
    "  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "  #print(user)\n",
    "  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color, thickness, display_str_list, use_normalized_coordinates, user=user)\n",
    "  np.copyto(image, np.array(image_pil))  # image flush\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):\n",
    "  if image.dtype != np.uint8:\n",
    "    raise ValueError('`image` not of type np.uint8')\n",
    "  if mask.dtype != np.uint8:\n",
    "    raise ValueError('`mask` not of type np.uint8')\n",
    "  if np.any(np.logical_and(mask != 1, mask != 0)):\n",
    "    raise ValueError('`mask` elements should be in [0, 1]')\n",
    "  if image.shape[:2] != mask.shape:\n",
    "    raise ValueError('The image has spatial dimensions %s, but the mask has dimensions %s' % (image.shape[:2], mask.shape))\n",
    "  rgb = ImageColor.getrgb(color)\n",
    "  pil_image = Image.fromarray(image)\n",
    "  solid_color = np.expand_dims(np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
    "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert('L')\n",
    "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "  np.copyto(image, np.array(pil_image.convert('RGB')))\n",
    "  \n",
    "def plot_mask(color, alpha, original_image, mask):  #! Simialr to draw_mask_on_image_array()\n",
    "  rgb = ImageColor.getrgb(color)\n",
    "  pil_image = Image.fromarray(original_image)\n",
    "\n",
    "  solid_color = np.expand_dims(np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
    "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
    "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert('L')\n",
    "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
    "  ## Maintain the original image\n",
    "  img_w_mask = np.array(pil_image.convert('RGB'))\n",
    "  return img_w_mask\n",
    "\n",
    "\n",
    "\n",
    "#* #########################################################\n",
    "#@ Visualize labeled boxes, scores\n",
    "#@ image: uint8 numpy array [img_height, img_width, 3]\n",
    "#@ boxes: numpy array [N, 4]\n",
    "#@ classes: numpy array [N]. Note that class indices are 1-based and match the keys in the label map.\n",
    "#@ scores: numpy array [N] or None(assumes plotted boxes are groundtruth boxes. Plot all boxes as black without classes/scores)\n",
    "#@ category_index: a dict [[category index `id`, category name `name`] keyed by category indices]\n",
    "global scores_all\n",
    "global indices_fg\n",
    "def visualize_boxes_and_labels_on_image_array(image, boxes, classes, scores, category_index,user,\n",
    "                                              instance_masks=None, instance_boundaries=None,\n",
    "                                              use_normalized_coordinates=False,\n",
    "                                              max_boxes_to_draw=20, min_score_thresh=.5, #max number of boxes and  minimum score to visualize\n",
    "                                              agnostic_mode=False,\n",
    "                                              line_thickness=4,\n",
    "                                              groundtruth_box_visualization_color='black',\n",
    "                                              skip_scores=False, skip_labels=False,\n",
    "                                              mask_alpha=0.4,\n",
    "                                              plot_color=None,):\n",
    "  box_to_display_str_map = collections.defaultdict(list)\n",
    "  box_to_color_map = collections.defaultdict(str)\n",
    "  box_to_instance_masks_map = {}\n",
    "  box_to_score_map = {}\n",
    "  box_to_instance_boundaries_map = {}\n",
    "  \n",
    "  #print(user)\n",
    "  \n",
    "  if not max_boxes_to_draw:\n",
    "    max_boxes_to_draw = boxes.shape[0]\n",
    "\n",
    "  #//print(scores)\n",
    "  #//print(\"boxes.shape: \", boxes.shape[0])\n",
    "  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "    print(i)\n",
    "    if scores is None or scores[i] > min_score_thresh:\n",
    "      box = tuple(boxes[i].tolist())\n",
    "      \n",
    "      if instance_masks is not None:\n",
    "        box_to_instance_masks_map[box] = instance_masks[i]\n",
    "      if instance_boundaries is not None:\n",
    "        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n",
    "      if scores is None:\n",
    "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
    "      \n",
    "      else:\n",
    "        display_str = ''\n",
    "        if not skip_labels:\n",
    "          if not agnostic_mode:\n",
    "            #print(scores_all[i])\n",
    "            #print(category_indices[np.argmax(scores_all[i])])\n",
    "            #print(category_indices)\n",
    "            #//class_name = category_index[np.argmax(scores_all[classes[i]])]['name']k\n",
    "            #//print(\"the number is: \", classes[i], \"\\n\",\"the possibility is: \", scores_all[classes[i]])\n",
    "            class_name = category_index[np.argmax(scores_all[indices_fg[i]])]['name']\n",
    "            #print(list(category_index.keys()))\n",
    "            #if classes[i] in list(category_index.keys()):\n",
    "              #print(classes[i])\n",
    "              #!class_name = category_index[classes[i]]['name']\n",
    "              #print(class_name)\n",
    "            #else:\n",
    "            #  class_name = 'N/A'\n",
    "            display_str = str(class_name)\n",
    "            #print(display_str)\n",
    "            \n",
    "        ## SKIP_LABELS IF ENDING\n",
    "        if not skip_scores:\n",
    "          #if not display_str:\n",
    "          #  display_str = '{}%'.format(int(100*scores[i]))\n",
    "          #else:\n",
    "          #  float_score = (\"%.2f\" % scores[i]).lstrip('0')\n",
    "          #  display_str = '{}: {}'.format(display_str, float_score)\n",
    "          float_score = (\"%.2f\" % float(np.max(scores_all[indices_fg[i]])*100))\n",
    "          display_str = '{}: {}%'.format(display_str, float_score)\n",
    "          box_to_score_map[box] = int(100*scores[i])\n",
    "          \n",
    "        ## SKIP_SCORES IF ENDING \n",
    "        box_to_display_str_map[box].append(display_str)\n",
    "        if plot_color is not None:\n",
    "          box_to_color_map[box] = plot_color\n",
    "        elif agnostic_mode:\n",
    "          box_to_color_map[box] = 'DarkOrange'\n",
    "        else:\n",
    "          box_to_color_map[box] = STANDARD_COLORS[\n",
    "              classes[i] % len(STANDARD_COLORS)]\n",
    "  \n",
    "  ## Handle the case when box_to_score_map is empty.\n",
    "  if box_to_score_map:\n",
    "    box_color_iter = sorted(box_to_color_map.items(), key=lambda kv: box_to_score_map[kv[0]])\n",
    "  else:\n",
    "    box_color_iter = box_to_color_map.items()\n",
    "\n",
    "  ## Draw all boxes onto image.\n",
    "  ## UI Change parameter: user ##\n",
    "  for box_color, tmpUser in zip(box_color_iter, user):\n",
    "    box, color = box_color\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    if instance_masks is not None:\n",
    "      draw_mask_on_image_array(image, box_to_instance_masks_map[box], color=color, alpha=mask_alpha)\n",
    "    if instance_boundaries is not None:\n",
    "      draw_mask_on_image_array(image, box_to_instance_boundaries_map[box], color='red', alpha=1.0)\n",
    "      \n",
    "    draw_bounding_box_on_image_array(image, ymin, xmin, ymax, xmax, user=tmpUser,\n",
    "                                     color=color, thickness=line_thickness,\n",
    "                                     display_str_list=box_to_display_str_map[box],\n",
    "                                     use_normalized_coordinates=use_normalized_coordinates)\n",
    "    \n",
    "    #print(type(image))\n",
    "    im = Image.fromarray(image)\n",
    "    im.save(\"tmp/ui.jpg\")\n",
    "    #//cv2.imwrite(\"tmp/ui.png\", image)\n",
    "  return image   #@ Return uint8 numpy array [img_height, img_width, 3] with overlaid boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37559953, 0.32875124],\n",
       "       [0.2853251 , 0.2450394 ],\n",
       "       [0.24868415, 0.21461299],\n",
       "       [0.27040896, 0.22903839],\n",
       "       [0.25045556, 0.23159668],\n",
       "       [0.24574862, 0.2140573 ],\n",
       "       [0.36556888, 0.31986576],\n",
       "       [0.23596096, 0.23321402],\n",
       "       [0.26907587, 0.23978895],\n",
       "       [0.275301  , 0.24211088],\n",
       "       [0.32001954, 0.2866404 ],\n",
       "       [0.23780736, 0.2143878 ],\n",
       "       [0.2884213 , 0.26526436],\n",
       "       [0.23272376, 0.21258655],\n",
       "       [0.36639145, 0.3003508 ],\n",
       "       [0.26842093, 0.22223848],\n",
       "       [0.25790298, 0.20088194],\n",
       "       [0.24730152, 0.21247607],\n",
       "       [0.23920766, 0.20116681],\n",
       "       [0.23907755, 0.22857589],\n",
       "       [0.29997778, 0.25150838],\n",
       "       [0.25656635, 0.22271009],\n",
       "       [0.25664788, 0.22105701],\n",
       "       [0.25040177, 0.21729903],\n",
       "       [0.2563432 , 0.2264043 ],\n",
       "       [0.31292802, 0.28262672],\n",
       "       [0.24536589, 0.2183201 ],\n",
       "       [0.26398167, 0.22470008],\n",
       "       [0.25014472, 0.22210796],\n",
       "       [0.30146694, 0.27224702],\n",
       "       [0.26527146, 0.22620358],\n",
       "       [0.27777702, 0.2553395 ],\n",
       "       [0.32373273, 0.29797027],\n",
       "       [0.246977  , 0.20709546],\n",
       "       [0.2415064 , 0.22416472],\n",
       "       [0.27542347, 0.25856218],\n",
       "       [0.28863138, 0.252978  ],\n",
       "       [0.23566464, 0.22123319],\n",
       "       [0.31215316, 0.2759204 ],\n",
       "       [0.25824285, 0.24014679],\n",
       "       [0.230925  , 0.24098752],\n",
       "       [0.25297844, 0.22862007],\n",
       "       [0.30216444, 0.27089512],\n",
       "       [0.24065997, 0.22868383],\n",
       "       [0.26732856, 0.23919103],\n",
       "       [0.24176158, 0.22788683],\n",
       "       [0.2443342 , 0.22015962],\n",
       "       [0.3083591 , 0.26322263],\n",
       "       [0.2510603 , 0.21336843],\n",
       "       [0.2779586 , 0.24058491]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@parameter debug\n",
    "#@indices #array([ 0, 14,  6, 32, 10, 25, 38, 47, 42, 29, 20, 36, 12,  1, 49, 31, 35,\n",
    "          #9,  3,  8, 15, 44, 30, 27, 39, 16, 22, 21, 24, 41, 48,  4, 23, 28,\n",
    "          #2, 17, 33,  5, 26, 46, 45, 34, 40, 43, 18, 19, 11,  7, 37, 13])\n",
    "#@indices_fg #array([40])\n",
    "#@valid_indices[:max_boxes_to_draw] #array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
    "                                    #17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
    "                                    #34, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START UI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 21:39:51.076 python[14419:314224] TSM AdjustCapsLockLEDForKeyTransitionHandling - _ISSetPhysicalKeyboardCapsLockLED Inhibit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person;car\n",
      "START DETECTING!\n",
      "Valid Indices Cnt:  254\n",
      "Building text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Update Detection Window\n",
      "car;person\n",
      "START DETECTING!\n",
      "Valid Indices Cnt:  254\n",
      "Building text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Update Detection Window\n",
      "car;person\n",
      "START DETECTING!\n",
      "Valid Indices Cnt:  254\n",
      "Building text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "Update Detection Window\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "car\n",
      "START DETECTING!\n",
      "Valid Indices Cnt:  254\n",
      "Building text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "Update Detection Window\n"
     ]
    }
   ],
   "source": [
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtCore import *\n",
    "from PyQt5.QtGui import *\n",
    "from PyQt5.QtWidgets import *\n",
    "import shutil\n",
    "print(\"START UI\")\n",
    "\n",
    "class firstWindow(QWidget):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(firstWindow, self).__init__(*args, **kwargs)\n",
    "    self.image_path = './examples/test.jpg'\n",
    "    self.setWindowTitle('Detect objects. Please input custom labels.')\n",
    "    self.currentText = \"\"\n",
    "    self.bboxMax = 0\n",
    "    self.category_name_string=\"\"\n",
    "    \n",
    "    self.img = cv2.imread(self.image_path)\n",
    "    self.imageHeight, self.imageWidth, self.bytesPerComponent = self.img.shape\n",
    "    self.resize(self.imageWidth, self.imageHeight)\n",
    "    self.move(int((QDesktopWidget().screenGeometry().width()-self.width())/2),\n",
    "              int((QDesktopWidget().screenGeometry().height()-self.height())/2))  \n",
    "    \n",
    "    self.lb_0 = originalLabel(self) \n",
    "    self.customLabel = customInputLabel(self)\n",
    "    self.bboxNumLabel = bboxInputLabel(self)\n",
    "    self.button_1_Init()\n",
    "    self.button_2_Init()\n",
    "    self.button_3_Init()\n",
    "    self.button_4_Init()\n",
    "    \n",
    "  def VisualImage(self):\n",
    "    #//print(self.image_path)\n",
    "    self.img = cv2.imread(self.image_path)\n",
    "    self.imageHeight, self.imageWidth, self.bytesPerComponent = self.img.shape\n",
    "    self.resize(self.imageWidth, self.imageHeight)\n",
    "    self.move(int((QDesktopWidget().screenGeometry().width()-self.width())/2),\n",
    "              int((QDesktopWidget().screenGeometry().height()-self.height())/2)) \n",
    "    self.lb_0.setGeometry(QRect(0, 0, self.imageWidth, self.imageHeight))\n",
    "    bytesPerLine = 3 * self.imageWidth\n",
    "    cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB, self.img)\n",
    "    QImg = QImage(self.img.data, self.imageWidth, self.imageHeight, bytesPerLine, QImage.Format_RGB888)\n",
    "    pixmap = QPixmap.fromImage(QImg)\n",
    "    self.lb_0.setPixmap(pixmap)\n",
    "    self.update()\n",
    "    \n",
    "  def textchange(self, text):\n",
    "    self.currentText = text\n",
    "  def numchange(self, bboxNumbers):\n",
    "    self.bboxMax = bboxNumbers\n",
    "    #//print(self.bboxMax)\n",
    "      \n",
    "  def showPath(self):\n",
    "    print(self.image_path)\n",
    "  \n",
    "  def button_1_Init(self):\n",
    "    self.bt = QPushButton(self)\n",
    "    self.bt.resize(30, 30)\n",
    "    self.bt.setStyleSheet(\"background-color:transparent\")\n",
    "    self.bt.setIcon(QIcon(\"icon/icons8-enter-mac-key-90.png\"))\n",
    "    self.bt.setIconSize(QSize(self.bt.size()))\n",
    "    self.bt.move(self.customLabel.x()+520, self.lb_0.y()+15)\n",
    "    self.bt.clicked.connect(self.clickInputCustomLabel)\n",
    "    \n",
    "  def clickInputCustomLabel(self):\n",
    "    self.msg_label = QMessageBox(self)\n",
    "    self.msg_label.setText(\"Get custom Labels :  \\n\" + self.currentText + \"\\nand Max bbox numbers : \\n\"+self.bboxMax)\n",
    "    self.msg_label.setIcon(QMessageBox.Information)\n",
    "    self.msg_label.show()\n",
    "    cusLabel = self.currentText.split(',')\n",
    "    self.category_name_string = ';'.join(cusLabel) ## + background \n",
    "    print(self.category_name_string)\n",
    "       \n",
    "  def button_2_Init(self):\n",
    "    self.bt_2 = QPushButton(self)\n",
    "    self.bt_2.resize(30, 30)\n",
    "    self.bt_2.setStyleSheet(\"background-color:transparent\")\n",
    "    self.bt_2.setIcon(QIcon(\"icon/play.png\"))\n",
    "    self.bt_2.setIconSize(QSize(self.bt_2.size()))\n",
    "    self.bt_2.move(self.bt.x()+40, self.bt.y())  \n",
    "    self.bt_2.clicked.connect(self.startDetecting)\n",
    "  \n",
    "  ## WITH VILD MODEL ##\n",
    "  def startDetecting(self):\n",
    "    print(\"START DETECTING!\")\n",
    "    global max_boxes_to_draw        ########\n",
    "    max_boxes_to_draw = int(self.bboxMax) \n",
    "    #//print(type(self.bboxMax))\n",
    "    nms_threshold = 0.6 \n",
    "    global min_rpn_score_thresh     ########\n",
    "    min_rpn_score_thresh = 0.9\n",
    "    min_box_area = 220\n",
    "    category_names = ['background']+[x.strip() for x in self.category_name_string.split(';')]\n",
    "    categories = [{'name': item, 'id': idx,} for idx, item in enumerate(category_names)]\n",
    "    global category_indices         ########\n",
    "    category_indices = {cat['id']: cat for cat in categories} \n",
    "    \n",
    "    roi_boxes, roi_scores, detection_boxes, scores_unused, box_outputs, detection_masks, visual_features, image_info \\\n",
    "    = session.run(\n",
    "      ['RoiBoxes:0', 'RoiScores:0', '2ndStageBoxes:0', '2ndStageScoresUnused:0', 'BoxOutputs:0', 'MaskOutputs:0', 'VisualFeatOutputs:0', 'ImageInfo:0'],\n",
    "      feed_dict={'Placeholder:0': [self.image_path, ]}\n",
    "    )\n",
    "    roi_boxes = np.squeeze(roi_boxes, axis=0) \n",
    "    roi_scores = np.squeeze(roi_scores, axis=0)\n",
    "    detection_boxes = np.squeeze(detection_boxes, axis=(0, 2))\n",
    "    scores_unused = np.squeeze(scores_unused, axis=0)\n",
    "    box_outputs = np.squeeze(box_outputs, axis=0)\n",
    "    detection_masks = np.squeeze(detection_masks, axis=0)\n",
    "    visual_features = np.squeeze(visual_features, axis=0)\n",
    "    image_info = np.squeeze(image_info, axis=0)\n",
    "    \n",
    "    image_scale = np.tile(image_info[2:3, :], (1, 2))\n",
    "    image_height = int(image_info[0, 0])\n",
    "    image_width = int(image_info[0, 1])\n",
    "    global rescaled_detection_boxes ########\n",
    "    rescaled_detection_boxes = detection_boxes / image_scale\n",
    "    global image                    ########\n",
    "    image = np.asarray(Image.open(open(self.image_path, 'rb')).convert(\"RGB\"))\n",
    "    assert image_height == image.shape[0]\n",
    "    assert image_width == image.shape[1]\n",
    "    \n",
    "    nmsed_indices = nms(detection_boxes, roi_scores, thresh=nms_threshold)\n",
    "    box_sizes = (rescaled_detection_boxes[:, 2] - rescaled_detection_boxes[:, 0]) * (rescaled_detection_boxes[:, 3] - rescaled_detection_boxes[:, 1])\n",
    "    global valid_indices            ########\n",
    "    valid_indices = np.where(\n",
    "      np.logical_and(\n",
    "        np.isin(np.arange(len(roi_scores), dtype=np.int64), nmsed_indices),\n",
    "          np.logical_and(\n",
    "            np.logical_not(np.all(roi_boxes == 0., axis=-1)),\n",
    "            np.logical_and(\n",
    "              roi_scores >= min_rpn_score_thresh,\n",
    "              box_sizes > min_box_area \n",
    "            ))))[0]\n",
    "    print('Valid Indices Cnt: ', len(valid_indices))\n",
    "    global detection_roi_scores     ########\n",
    "    detection_roi_scores = roi_scores[valid_indices][:max_boxes_to_draw, ...]\n",
    "    detection_boxes = detection_boxes[valid_indices][:max_boxes_to_draw, ...]\n",
    "    detection_masks = detection_masks[valid_indices][:max_boxes_to_draw, ...]\n",
    "    detection_visual_feat = visual_features[valid_indices][:max_boxes_to_draw, ...]\n",
    "    rescaled_detection_boxes = rescaled_detection_boxes[valid_indices][:max_boxes_to_draw, ...]    \n",
    "    \n",
    "    global isSaved                  ########\n",
    "    isSaved = np.zeros(detection_roi_scores.shape)\n",
    "    \n",
    "    text_features = build_text_embedding(categories)\n",
    "    raw_scores = detection_visual_feat.dot(text_features.T)\n",
    "    global scores_all               ########\n",
    "    if FLAGS.use_softmax:\n",
    "      scores_all = softmax(FLAGS.temperature * raw_scores, axis=-1)\n",
    "    else:\n",
    "      scores_all = raw_scores\n",
    "    #//print(scores_all.shape)\n",
    "    #* RANK\n",
    "    global indices                  ########\n",
    "    indices = np.argsort(-np.max(scores_all, axis=1))\n",
    "    global indices_fg               ########\n",
    "    indices_fg = np.array([i for i in indices if np.argmax(scores_all[i]) != 0])  \n",
    "    global indices_fg_2\n",
    "    indices_fg_2 = np.intersect1d(indices_fg, valid_indices)\n",
    "    \n",
    "    ymin, xmin, ymax, xmax = np.split(rescaled_detection_boxes, 4, axis=-1)\n",
    "    processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)\n",
    "    global segmentations            ########\n",
    "    segmentations = paste_instance_masks(detection_masks, processed_boxes, image_height, image_width) \n",
    "    \n",
    "    if len(indices_fg) == 0:\n",
    "      self.mgb_no = QMessageBox(self)\n",
    "      self.mgb_no.setIcon(QMessageBox.Information)\n",
    "      self.mgb_no.setText(\"ViLD does not detect anything belong to the given category\")\n",
    "      self.mgb_no.show()\n",
    "      print('ViLD does not detect anything belong to the given category')\n",
    "    else:\n",
    "      image_with_detections = visualize_boxes_and_labels_on_image_array( \\\n",
    "        np.array(image),\n",
    "        rescaled_detection_boxes[indices_fg],\n",
    "        valid_indices[:max_boxes_to_draw][indices_fg],\n",
    "        detection_roi_scores[indices_fg],   \n",
    "        category_indices, #numbered_category_indices,\n",
    "        user=isSaved[indices_fg], \n",
    "        instance_masks=segmentations[indices_fg],\n",
    "        use_normalized_coordinates=False,\n",
    "        max_boxes_to_draw=max_boxes_to_draw, min_score_thresh=min_rpn_score_thresh,\n",
    "        skip_scores=False, skip_labels=False,\n",
    "      )\n",
    "      #//plt.imshow(image_with_detections)  \n",
    "      \n",
    "      print(\"Update Detection Window\")\n",
    "      win_2 = resultWindow(self)\n",
    "      win_2.show()\n",
    "    \n",
    "  def button_3_Init(self):   \n",
    "    self.bt_3 = QPushButton(self)\n",
    "    self.bt_3.resize(25, 25)\n",
    "    self.bt_3.setIcon(QIcon(\"icon/icons8-image-document-64\"))\n",
    "    self.bt_3.setStyleSheet(\"background-color:transparent\")\n",
    "    self.bt_3.setIconSize(QSize(self.bt_3.size()))\n",
    "    self.bt_3.move(self.bt_2.x()+40, self.bt.y())\n",
    "    self.bt_3.clicked.connect(self.clickChangeImage)\n",
    "  \n",
    "  def clickChangeImage(self):\n",
    "    #//print(\"Clicked!\")\n",
    "    QFDialog = QFileDialog()\n",
    "    image_choose, _ = QFDialog.getOpenFileName(self, \n",
    "        \"Choose a new image to detect.\",\n",
    "        os.getcwd() + \"/examples\",\n",
    "        \"All Files (*);;Image Files (*.jpg)\")\n",
    "    if image_choose == \"\":\n",
    "      print(\"NO IMAGE CHOSEN!\")\n",
    "      return\n",
    "    print(\"New Iamge: \", image_choose, \"\\n\")\n",
    "    image_choose = image_choose.split('/')[-1]\n",
    "    self.image_path = './examples/'+ image_choose\n",
    "    #//self.showPath() \n",
    "    self.VisualImage()\n",
    "     \n",
    "  def button_4_Init(self):   \n",
    "    self.bt_4 = QPushButton(self)\n",
    "    self.bt_4.resize(25, 25)\n",
    "    self.bt_4.setIcon(QIcon(\"icon/icons8-save-50.png\"))\n",
    "    self.bt_4.setStyleSheet(\"background-color:transparent\")\n",
    "    self.bt_4.setIconSize(QSize(self.bt_4.size()))\n",
    "    self.bt_4.move(self.bt_3.x()+40, self.bt.y())\n",
    "    self.bt_4.clicked.connect(self.saveDetectedRes) \n",
    "  \n",
    "  def saveDetectedRes(self):\n",
    "    shutil.rmtree(\"./result\")\n",
    "    os.mkdir('result')\n",
    "    cnt = 0\n",
    "    global image                    ########\n",
    "    global rescaled_detection_boxes ########\n",
    "    raw_image = np.array(image)\n",
    "    n_boxes = rescaled_detection_boxes.shape[0]\n",
    "    global indices                  ########\n",
    "    global detection_roi_scores     ########\n",
    "    global scores_all               ########\n",
    "    global isSaved                  ########\n",
    "    for anno_idx in indices[0:int(n_boxes)]:\n",
    "      print(rescaled_detection_boxes)\n",
    "      rpn_score = detection_roi_scores[anno_idx]\n",
    "      bbox = rescaled_detection_boxes[anno_idx]  ## Get BBox\n",
    "      scores = scores_all[anno_idx]\n",
    "      showOrNot = isSaved[anno_idx]\n",
    "      \n",
    "      if np.argmax(scores) == 0:\n",
    "        continue\n",
    "      if showOrNot==2 or showOrNot==3: # if showOrNot==0 or showOrNot==2:\n",
    "        continue\n",
    "      \n",
    "      y1, x1, y2, x2 = int(np.floor(bbox[0])), int(np.floor(bbox[1])), int(np.ceil(bbox[2])), int(np.ceil(bbox[3]))\n",
    "      img_w_mask = plot_mask(mask_color, alpha, raw_image, segmentations[anno_idx])\n",
    "      crop_w_mask = img_w_mask[y1:y2, x1:x2, :]\n",
    "      \n",
    "      category_names = ['background']+[x.strip() for x in self.category_name_string.split(';')] \n",
    "      fig_size_h = min(max(5, int(len(category_names)/2.5)), 10)\n",
    "      fig, axs = plt.subplots(\n",
    "        1, 4, figsize=(fig_size_w, fig_size_h),\n",
    "        gridspec_kw={'width_ratios': [3, 1, 1, 2]}, \n",
    "        constrained_layout=True\n",
    "      )\n",
    "      #@ Draw bounding box.\n",
    "      rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=line_thickness, edgecolor='r', facecolor='none')\n",
    "      axs[0].add_patch(rect)\n",
    "      axs[0].set_xticks([])\n",
    "      axs[0].set_yticks([])\n",
    "      axs[0].set_title(f'bbox: {y1, x1, y2, x2}   area: {(y2 - y1) * (x2 - x1)}   rpn score: {rpn_score:.4f}')\n",
    "      axs[0].imshow(raw_image)\n",
    "      #@ Draw image in a cropped region.\n",
    "      crop = np.copy(raw_image[y1:y2, x1:x2, :])\n",
    "      axs[1].set_xticks([])\n",
    "      axs[1].set_yticks([])\n",
    "      axs[1].set_title(f'predicted: {category_names[np.argmax(scores)]}')\n",
    "      showLabel = category_names[np.argmax(scores)]\n",
    "      axs[1].imshow(crop)\n",
    "      #@ Draw segmentation inside a cropped region.\n",
    "      axs[2].set_xticks([])\n",
    "      axs[2].set_yticks([])\n",
    "      axs[2].set_title('mask')\n",
    "      axs[2].imshow(crop_w_mask)\n",
    "      #@ Draw category scores.\n",
    "      fontsize = max(min(fig_size_h/float(len(category_names))*45, 20), 8)\n",
    "      for cat_idx in range(len(category_names)):\n",
    "        axs[3].barh(cat_idx, scores[cat_idx], \n",
    "                    color='orange' if scores[cat_idx] == max(scores) else 'blue')\n",
    "      axs[3].invert_yaxis()\n",
    "      axs[3].set_axisbelow(True)\n",
    "      axs[3].set_xlim(0, 1)\n",
    "      plt.xlabel(\"confidence score\")\n",
    "      axs[3].set_yticks(range(len(category_names)))\n",
    "      axs[3].set_yticklabels(category_names, fontdict={'fontsize': fontsize})\n",
    "\n",
    "      f = plt.gcf()\n",
    "      f.savefig('result/result_{id}.jpg'.format(id=anno_idx), bbox_inches=\"tight\")\n",
    "      f.clear()\n",
    "      cnt += 1\n",
    "    self.msg_label = QMessageBox(self)\n",
    "    self.msg_label.setText(\"RESULTS SAVED!\")\n",
    "    self.msg_label.setInformativeText(\"Permitted Detection Counts: \" + str(cnt) + '!')\n",
    "    self.msg_label.setIcon(QMessageBox.Information)\n",
    "    self.msg_label.show()\n",
    "  \n",
    "class originalLabel(QLabel):\n",
    "  def __init__(self, parent=None):\n",
    "    super(originalLabel, self).__init__((parent))\n",
    "    self.parentWindow = parent\n",
    "    self.setGeometry(QRect(0, 0, self.parentWindow.imageWidth, self.parentWindow.imageHeight))\n",
    "    \n",
    "    bytesPerLine = 3 * self.parentWindow.imageWidth\n",
    "    cv2.cvtColor(self.parentWindow.img, cv2.COLOR_BGR2RGB, self.parentWindow.img)\n",
    "    QImg = QImage(self.parentWindow.img.data, self.parentWindow.imageWidth, self.parentWindow.imageHeight, bytesPerLine, QImage.Format_RGB888)\n",
    "    pixmap = QPixmap.fromImage(QImg)\n",
    "    self.setPixmap(pixmap)\n",
    "    \n",
    "class customInputLabel(QLineEdit):\n",
    "  def __init__(self, parent=None):\n",
    "    super(customInputLabel, self).__init__((parent))\n",
    "    self.parentWindow = parent\n",
    "    \n",
    "    re = QRegExp('[a-zA-Z,]+$')\n",
    "    re_validato = QRegExpValidator(re)\n",
    "    self.setValidator(re_validato)\n",
    "    self.move(self.parentWindow.lb_0.x()+50, self.parentWindow.lb_0.y()+20)\n",
    "    self.resize(510, 20)\n",
    "    self.setText(\"Please input your custom label(s) to Zero-shot Detect! [separated by',']\")\n",
    "    self.textChanged.connect(self.parentWindow.textchange) \n",
    "    \n",
    "class bboxInputLabel(QLineEdit):\n",
    "  def __init__(self, parent=None):\n",
    "    super(bboxInputLabel, self).__init__((parent))\n",
    "    self.parentWindow = parent\n",
    "    re = QRegExp('[0-9]+$')\n",
    "    re_validato = QRegExpValidator(re)\n",
    "    self.setValidator(re_validato)\n",
    "    self.move(self.parentWindow.lb_0.x()+50, self.parentWindow.lb_0.y()+45)\n",
    "    self.resize(160, 20)\n",
    "    self.setText(\"Max bbox numbers:\")\n",
    "    self.textChanged.connect(self.parentWindow.numchange)   \n",
    "    \n",
    "    \n",
    "class resultWindow(QMainWindow):\n",
    "  def __init__(self, parent=None):\n",
    "    super(resultWindow, self).__init__(parent)\n",
    "    self.parentWindow = parent\n",
    "    self.setWindowTitle('Detected objects. Zero-shot ends.')\n",
    "    img = cv2.imread(\"tmp/ui.jpg\")\n",
    "    self.imageHeight, self.imageWidth, self.bytesPerComponent = img.shape\n",
    "    self.resize(self.imageWidth, self.imageHeight)\n",
    "    self.move(int((QDesktopWidget().screenGeometry().width()-self.width())/2),\n",
    "              int((QDesktopWidget().screenGeometry().height()-self.height())/2))\n",
    "        \n",
    "    self.lb = resultLabel(self)\n",
    "    self.lb.setGeometry(QRect(0, 0, self.imageWidth, self.imageHeight))\n",
    "    bytesPerLine = 3 * self.imageWidth\n",
    "    cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n",
    "    QImg = QImage(img.data, self.imageWidth, self.imageHeight, bytesPerLine, QImage.Format_RGB888)\n",
    "    pixmap = QPixmap.fromImage(QImg)\n",
    "    self.lb.setPixmap(pixmap)\n",
    "    self.lb.setCursor(Qt.CrossCursor)\n",
    "\n",
    "class resultLabel(QLabel):\n",
    "  def __init__(self, parent=None):\n",
    "    super(resultLabel, self).__init__((parent))   \n",
    "    self.parentWindow = parent\n",
    "    self.rootWindow = self.parentWindow.parentWindow\n",
    "    self.x0 = 0\n",
    "    self.y0 = 0\n",
    "    self.x1 = 0\n",
    "    self.y1 = 0\n",
    "    self.flag = False\n",
    "    self.index = 0\n",
    "\n",
    "  def mousePressEvent(self, event):\n",
    "    self.flag = True\n",
    "    self.x0 = event.x()\n",
    "    self.y0 = event.y()\n",
    "    #//print(self.x0, self.y0)\n",
    "    self.index = 0\n",
    "    ## TODO Area Cverlap ## \n",
    "    global indices_fg               ########\n",
    "    global rescaled_detection_boxes ########\n",
    "    global isSaved                  ########\n",
    "    for self.index in indices_fg:\n",
    "      #//print(self.index)\n",
    "      ymin, xmin, ymax, xmax = rescaled_detection_boxes[self.index]\n",
    "      #//print(ymin, xmin, ymax, xmax)\n",
    "      if self.x0 < xmax and self.x0 > xmin and self.y0 < ymax and self.y0 > ymin and isSaved[self.index]!=3:\n",
    "        self.cb = QComboBox(self)\n",
    "        self.cb.move(self.x0, self.y0)\n",
    "        self.cb.addItem('This bbox is selected')\n",
    "        self.cb.addItem(QtGui.QIcon(\"./icon/icons8-assessments-90.png\"),'Get its results')\n",
    "        self.cb.addItem(QtGui.QIcon(\"./icon/icons8-multiplication-90.png\"),'Discard it')\n",
    "        self.cb.addItem(QtGui.QIcon(\"./icon/icons8-garbage-66.png\"),'Discard it FOREVER')\n",
    "        self.cb.setCurrentIndex(0)\n",
    "        \n",
    "        self.cb.currentIndexChanged[str].connect(self.oper)\n",
    "        self.cb.show()\n",
    "        break\n",
    "      \n",
    "  def oper(self, choice):\n",
    "    global image                    ########\n",
    "    global valid_indices            ########\n",
    "    global max_boxes_to_draw        ########\n",
    "    global detection_roi_scores     ########\n",
    "    global category_indices         ########\n",
    "    global segmentations            ########\n",
    "    global min_rpn_score_thresh     ########\n",
    "    #//print(i)\n",
    "    if choice == \"Get its results\":\n",
    "      #//print(self.index)\n",
    "      # Global Variable\n",
    "      isSaved[self.index] = 1\n",
    "      self.mgb = QMessageBox(self)\n",
    "      self.mgb.setIcon(QMessageBox.Information)\n",
    "      self.mgb.setText(\"You have saved the detection result.\")\n",
    "      self.mgb.show()\n",
    "      \n",
    "    elif choice == \"Discard it\":\n",
    "      #//print(self.index)\n",
    "      # Global Variable\n",
    "      isSaved[self.index] = 2\n",
    "      self.mgb = QMessageBox(self)\n",
    "      self.mgb.setIcon(QMessageBox.Warning)\n",
    "      self.mgb.setText(\"You would ignore this bbox!\")\n",
    "      self.mgb.setInformativeText(\"Later, it will not be saved\")\n",
    "      self.mgb.show()\n",
    "    \n",
    "    elif choice == \"Discard it FOREVER\":\n",
    "      isSaved[self.index] = 3\n",
    "      self.mgb = QMessageBox(self)\n",
    "      self.mgb.setIcon(QMessageBox.Warning)\n",
    "      self.mgb.setText(\"Discard FOREVER\")\n",
    "      self.mgb.setInformativeText(\"CANNOT RECOVER!\")\n",
    "      self.mgb.show()\n",
    "      \n",
    "    self.cb.deleteLater()\n",
    "    # Update Results\n",
    "    image_with_detections = visualize_boxes_and_labels_on_image_array( \\\n",
    "      np.array(image),\n",
    "      rescaled_detection_boxes[indices_fg],\n",
    "      valid_indices[:max_boxes_to_draw][indices_fg],\n",
    "      detection_roi_scores[indices_fg],   \n",
    "      category_indices, #numbered_category_indices,\n",
    "      user=isSaved[indices_fg], \n",
    "      instance_masks=segmentations[indices_fg],\n",
    "      use_normalized_coordinates=False,\n",
    "      max_boxes_to_draw=max_boxes_to_draw, min_score_thresh=min_rpn_score_thresh,\n",
    "      skip_scores=False, skip_labels=False,\n",
    "    )\n",
    "    \n",
    "    img = cv2.imread(\"tmp/ui.jpg\")\n",
    "    height, width, bytesPerComponent = img.shape\n",
    "    self.setGeometry(QRect(0, 0, width, height))\n",
    "    bytesPerLine = 3 * width\n",
    "    cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n",
    "    QImg = QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888)\n",
    "    pixmap = QPixmap.fromImage(QImg)\n",
    "    self.setPixmap(pixmap)\n",
    "    self.setCursor(Qt.CrossCursor)\n",
    "    \n",
    "  def mouseReleaseEvent(self,event):\n",
    "    self.flag = False\n",
    "\n",
    "\n",
    "####### Main Function ######\n",
    "def main():\n",
    "  app = QApplication(sys.argv)\n",
    "  win_1 = firstWindow()\n",
    "  \n",
    "  win_1.show()\n",
    "  app.exec_()\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('zeroShot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86c8192e3b52dd5b61cf7d633d9cde44ca62fe25113112c353ce106148b537f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
